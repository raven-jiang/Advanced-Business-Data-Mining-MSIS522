{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"name":"MSIS522-lab3-ensemble-learning-Solution.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"kWzzbRUANo0E","colab_type":"text"},"source":["# Ensemble Learning\n","\n","In this lab, you will practice how to use and fine-tune decision tree, random forest, adaboost, gradient boosted trees in scikit-learn. You will also get yourselves familiar with the hyper-parameters for each model in their API."]},{"cell_type":"code","metadata":{"id":"Y9_YMyCONo0F","colab_type":"code","colab":{}},"source":["# import packages\n","%matplotlib inline\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","import seaborn as sns\n","import pandas as pd\n","from sklearn.datasets import make_moons\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.utils import resample\n","\n","# make this notebook's output stable across runs\n","np.random.seed(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sj24GCsqNo0G","colab_type":"code","colab":{}},"source":["# helper functions used in this lab\n","def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True):\n","    \"\"\"\n","    Plot the decision boundary of a learnt classifier\n","    \"\"\"\n","    x1s = np.linspace(axes[0], axes[1], 100)\n","    x2s = np.linspace(axes[2], axes[3], 100)\n","    x1, x2 = np.meshgrid(x1s, x2s)\n","    X_new = np.c_[x1.ravel(), x2.ravel()]\n","    y_pred = clf.predict(X_new).reshape(x1.shape)\n","    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n","    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n","    if contour:\n","        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n","        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=1)\n","    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n","    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n","    plt.axis(axes)\n","    plt.xlabel(r\"$x_1$\", fontsize=18)\n","    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nuYr70omNo0I","colab_type":"text"},"source":["## Synthetic Moon dataset\n","In this lab, we will classify a moon shaped synthetic dataset with two features (_x1_ and _x2_) and two classes (0 or 1) with some added noise. "]},{"cell_type":"code","metadata":{"id":"Ck4I3NBoNo0J","colab_type":"code","colab":{}},"source":["# load the moon train and test data from CSV files\n","train = pd.read_csv(\"https://raw.githubusercontent.com/zariable/data/master/halfmoon_train.csv\")\n","test = pd.read_csv(\"https://raw.githubusercontent.com/zariable/data/master/halfmoon_test.csv\")\n","\n","train_x = train.iloc[:,0:2]\n","train_y = train.iloc[:,2]\n","\n","test_x = test.iloc[:,0:2]\n","test_y = test.iloc[:,2]\n","\n","print(\"Number of train data: {}\".format(len(train_y)))\n","print(\"Number of test data: {}\".format(len(test_y)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QAq6xEhWNo0L","colab_type":"code","colab":{}},"source":["# plot the train dataset\n","plt.scatter(train_x.x1, train_x.x2, s=40, c=train_y, cmap=plt.cm.Spectral)\n","plt.title('Scatter plot of the train dataset')\n","plt.xlabel('x1')\n","plt.ylabel('x2')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_oK91EX-No0N","colab_type":"code","colab":{}},"source":["# plot the test dataset\n","plt.scatter(test_x.x1, test_x.x2, s=40, c=test_y, cmap=plt.cm.Spectral)\n","plt.title('Scatter plot of the test dataset')\n","plt.xlabel('x1')\n","plt.ylabel('x2')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vil1ZPk_No0P","colab_type":"text"},"source":["## Decision Tree Classifier\n","\n","First, we learn a decision tree classifier to separate the two classes in the moon dataset. Note that decision boundary is axis-parallel."]},{"cell_type":"code","metadata":{"id":"5SqHBwjRNo0P","colab_type":"code","colab":{}},"source":["# Build Decision Tree classifier\n","parameters = {\n","    \"max_depth\": [2, 4], \n","    \"min_samples_split\": [0.05, 0.1, 0.2]\n","}\n","\n","dtc = DecisionTreeClassifier()\n","dtc_grid = GridSearchCV(dtc, parameters, cv=3)\n","dtc_grid.fit(train_x, train_y)\n","\n","# summarize the results of the grid search\n","print(\"The best score is {}\".format(dtc_grid.best_score_))\n","print(\"The best hyper parameter setting is {}\".format(dtc_grid.best_params_))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BOPgb0huNo0R","colab_type":"code","colab":{}},"source":["# plot the decision boundary\n","plot_decision_boundary(dtc_grid, train_x.values, train_y.values)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNX7nY3DNo0T","colab_type":"code","colab":{}},"source":["# make prediction and evaluate the model performance on test data\n","test_z = dtc_grid.predict(test_x)\n","test_z_prob = dtc_grid.predict_proba(test_x)[:,1]\n","\n","print(\"model accuracy: {}\".format(accuracy_score(test_y, test_z)))\n","print(\"model ROC AUC: {}\".format(roc_auc_score(test_y, test_z_prob)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N3h6YRtmNo0U","colab_type":"text"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"Gdp2fTQPNo0V","colab_type":"text"},"source":["## Random Forest\n","\n","<span style=\"color:orange\">**In this exercise, we will apply the Random Forest Classifier provided in scikit-learn to classify the synthetic moon data.**"]},{"cell_type":"markdown","metadata":{"id":"noYrdz9NNo0V","colab_type":"text"},"source":["A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default). Please refer to the [scikit-learn doc](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for more details on how to use the API."]},{"cell_type":"code","metadata":{"id":"YTmWrt3MNo0W","colab_type":"code","colab":{}},"source":["# TODO: fine-tune Random Forest classifier using grid search with cross-validation (GridSearchCV).\n","parameters = {\n","    \"n_estimators\": [20, 40],\n","    \"max_depth\": [2, 4], \n","    \"min_samples_split\": [0.05, 0.1, 0.2]\n","}\n","\n","rfc_grid = GridSearchCV(RandomForestClassifier(n_jobs=-1, random_state=0), parameters, cv=3)\n","rfc_grid.fit(train_x, train_y)\n","\n","# summarize the results of the grid search\n","print(\"The best score is {}\".format(rfc_grid.best_score_))\n","print(\"The best hyper parameter setting is {}\".format(rfc_grid.best_params_))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b14koY5fNo0X","colab_type":"code","colab":{}},"source":["# plot the decision boundary\n","plot_decision_boundary(rfc_grid, train_x.values, train_y.values)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DHU2Pw-ZNo0Z","colab_type":"text"},"source":["Notice that the decision boundary is no longer orthogonal to the x and y axises and is able to better separate the two classes in the moon dataset.\n","\n","Now let's use the fine-tuned model to make predictions on the test dataset and evaluate it's performance in terms of accuracy and ROC AUC. It should outperform our simplified version of Random Forest as there are more optimizations being implemented in scikit-learn API."]},{"cell_type":"code","metadata":{"id":"pfAGyHJ0No0Z","colab_type":"code","colab":{}},"source":["# TODO: make prediction and evaluate the model performance on test data\n","test_z = rfc_grid.predict(test_x)\n","test_z_prob = rfc_grid.predict_proba(test_x)[:,1]\n","\n","print(\"model accuracy: {}\".format(accuracy_score(test_y, test_z)))\n","print(\"model ROC AUC: {}\".format(roc_auc_score(test_y, test_z_prob)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oi3KXvQZNo0b","colab_type":"text"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"PSjWZhcGNo0c","colab_type":"text"},"source":["## AdaBoost\n","<span style=\"color:orange\">**In this exercise, we will apply AdaBoost classifier in scikit-learn to classify the synthetic moon dataset.**</span> "]},{"cell_type":"markdown","metadata":{"id":"TT1_NgOtNo0c","colab_type":"text"},"source":["### Train AdaBoost model\n","[Adaboost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) algorithm works by learning each base learner so that it corrects the mis-classified data points from previous classifiers, and use weighted voting of predictions from each base learners for the final prediction. \n","\n","There are two key hyper-parameters:\n","- n_estimators: The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n","- learning_rate: Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators.\n"]},{"cell_type":"code","metadata":{"id":"qAvBvFFYNo0d","colab_type":"code","colab":{}},"source":["# TODO: fine-tune Adaboost with decision tree (max_depth=4) as base learners using grid search with cross-validation (GridSearchCV).\n","parameters = {\n","    \"n_estimators\": [20, 40],\n","    \"learning_rate\": [0.01, 0.1, 1, 10]\n","}\n","\n","adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=4), random_state=0)\n","adaboost_grid = GridSearchCV(adaboost, parameters, cv=3)\n","adaboost_grid.fit(train_x, train_y)\n","\n","# summarize the results of the grid search\n","print(\"The best score is {}\".format(adaboost_grid.best_score_))\n","print(\"The best hyper parameter setting is {}\".format(adaboost_grid.best_params_))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DH_AcFQeNo0e","colab_type":"text"},"source":["Visualize the decision boundaries."]},{"cell_type":"code","metadata":{"id":"lDj0S__9No0f","colab_type":"code","colab":{}},"source":["# plot the decision boundary\n","plot_decision_boundary(adaboost_grid, train_x.values, train_y.values)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XN7p4DwwNo0g","colab_type":"text"},"source":["Now let's use the fine-tuned model to make predictions on the test dataset and evaluate its performance in terms of accuracy and ROC AUC."]},{"cell_type":"code","metadata":{"id":"tkeGgCenNo0h","colab_type":"code","colab":{}},"source":["# TODO: make prediction and evaluate the model performance on test data\n","test_z = adaboost_grid.predict(test_x)\n","test_z_prob = adaboost_grid.predict_proba(test_x)[:,1]\n","\n","print(\"model accuracy: {}\".format(accuracy_score(test_y, test_z)))\n","print(\"model ROC AUC: {}\".format(roc_auc_score(test_y, test_z_prob)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1le0TufSNo0i","colab_type":"text"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"oghpcIpKNo0j","colab_type":"text"},"source":["## Gradient Boosted Trees\n","<span style=\"color:orange\">**In this exercise, we will apply the Gradient Boosted Tree classifier provided in scikit-learn to classify the synthetic moon dataset.**</span> "]},{"cell_type":"markdown","metadata":{"id":"3XHfNIZhNo0j","colab_type":"text"},"source":["Gradient Boosted Trees is a generalization of boosting to arbitrary differentiable loss functions, which can be used for both regression and classification problems. At a high level, gradient boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. Gradient Boosted Trees, which is a special case of gradient boosting, uses regression trees as the base predictors and builds an additive model in a forward stage-wise fashion. In each stage, regression trees are fit on the negative gradient of the loss function used in a classification or a regression problem. Please refer to [Gradient Boosted Trees](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) for more details on how to use the API.\n","\n","Gradient boosting introduces a new hyperparameter *learning_rate*, which scales the contribution of each tree. If you set it to a low values, such as 0.1, you will need more trees in the ensemble to fit the training data, but the predictions will usually generalize better. This is a regularization technique called shrinkage. In the exercise below, please include *learning_rate* as a hyperparameter in the model fine-tuning."]},{"cell_type":"code","metadata":{"id":"6VGlsCUXNo0k","colab_type":"code","colab":{}},"source":["# TODO: fine-tune Gradient Boosted Trees using grid search with cross-validation (GridSearchCV).\n","parameters = {\n","    \"loss\":[\"deviance\"],\n","    \"learning_rate\": [0.01, 0.1],\n","    \"min_samples_split\": [0.05, 0.1, 0.2],\n","    \"max_depth\":[2, 4],\n","    \"n_estimators\":[100]\n","}\n","\n","gbc_grid = GridSearchCV(GradientBoostingClassifier(), parameters, cv=3, n_jobs=-1)\n","gbc_grid.fit(train_x, train_y)\n","\n","# summarize the results of the grid search\n","print(\"The best score is {}\".format(gbc_grid.best_score_))\n","print(\"The best hyper parameter setting is {}\".format(gbc_grid.best_params_))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IkUqu0Q4No0m","colab_type":"text"},"source":["Visualize the decision boundaries."]},{"cell_type":"code","metadata":{"id":"BqH5oQCPNo0n","colab_type":"code","colab":{}},"source":["# plot the decision boundary for decision tree classifier\n","plot_decision_boundary(gbc_grid, train_x.values, train_y.values)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XBVukfZbNo0p","colab_type":"text"},"source":["Now let's use the fine-tuned model to make predictions on the test dataset and evaluate its performance in terms of accuracy and ROC AUC."]},{"cell_type":"code","metadata":{"id":"C3lWdGmsNo0p","colab_type":"code","colab":{}},"source":["# TODO: make prediction and evaluate the model performance on test data\n","test_z = gbc_grid.predict(test_x)\n","test_z_prob = gbc_grid.predict_proba(test_x)[:,1]\n","\n","print(\"model accuracy: {}\".format(accuracy_score(test_y, test_z)))\n","print(\"model ROC AUC: {}\".format(roc_auc_score(test_y, test_z_prob)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6WOfdxYENo0r","colab_type":"text"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"K-ODH-dMNo0r","colab_type":"text"},"source":["## Implement a simplified version of Random Forest Classifier\n","\n","In this exercise, you will implement the random forest classifier. \n","- Use bootstrapping to generate different ensemble datasets, where you will train a separate classifier to each dataset.\n","- Apply grid search with cross-validation to learn and fine-tune each decision tree classifier.\n","- Implement two predict functions: one outputs the probabilities while the other outputs the predicted class."]},{"cell_type":"code","metadata":{"id":"9PZNwHnKNo0s","colab_type":"code","colab":{}},"source":["class MyRandomForestClassifier:\n","    \"\"\"\n","    Random Forest Classifier.\n","    \"\"\"\n","    def __init__(self, n_estimators=100, models=None):\n","        self.n_estimators = n_estimators\n","        self.models = models\n","\n","    def train(self, x, y):\n","        \"\"\"\n","        TODO: Train random forest classifier using bootstrapping and decision tree as the base classifier\n","        \"\"\"\n","        self.models = [None] * self.n_estimators\n","        for i in range(self.n_estimators):\n","            # boostrapping\n","            if i % 10 == 0:\n","                print(\"Bootstrapping for estimator {}.\".format(i))\n","            x, y = resample(train_x.values, train_y, replace=True)\n","\n","            # build decision tree\n","            parameters = {\n","              \"max_depth\": [2, 4],\n","              \"min_samples_split\": [0.05, 0.1, 0.2]\n","            }\n","\n","            dtc_grid = GridSearchCV(DecisionTreeClassifier(), parameters, cv=3)\n","            dtc_grid.fit(x, y)\n","\n","            self.models[i] = dtc_grid\n","\n","    def predict_proba(self, x):\n","        \"\"\"\n","        TODO: Make predictions (probabilities) with the trained random forest\n","        \"\"\"\n","        if self.models == None:\n","            sys.exit(\"The model has not been trained yet. Please call train() first. Exiting...\")\n","\n","        predictions = pd.DataFrame(np.zeros((x.shape[0], self.n_estimators)))\n","        for i in range(self.n_estimators):\n","            predictions.iloc[:, i] = self.models[i].predict(x)\n","\n","        return predictions.sum(axis=1) / self.n_estimators\n","\n","    def predict(self, x):\n","        \"\"\"\n","        TODO: Make predictions (the class/label) with the trained random forest\n","        \"\"\"\n","        probabilities = self.predict_proba(x)\n","        prediction = pd.Series([1] * len(probabilities))\n","        prediction[probabilities < 0.5] = 0\n","\n","        return prediction.values"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cv_Ovq1ENo0t","colab_type":"text"},"source":["### Train, predict and evaluate the random forest classifier\n","\n","Even though this is a simplified version of random forest, we should see it outperforms the decision tree classifier above."]},{"cell_type":"code","metadata":{"id":"kC75yIlrNo0u","colab_type":"code","colab":{}},"source":["# Use the trained random forest model to make predictions on the test data and evaluate the model performance.\n","\n","# train random forest classifier\n","mrfc = MyRandomForestClassifier(100)\n","mrfc.train(train_x, train_y)\n","\n","# make predictions with the trained random forest\n","test_z = mrfc.predict(test_x)\n","test_z_prob = mrfc.predict_proba(test_x)\n","\n","# evaluate the model performance\n","print(\"model accuracy: {}\".format(accuracy_score(test_y, test_z)))\n","print(\"model ROC AUC: {}\".format(roc_auc_score(test_y, test_z_prob)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HiKHBmDwNo0v","colab_type":"text"},"source":["## Conclusion\n","\n","By comparing the model performance as well as the decision boundaries learned from different classifiers on the same dataset, we clearly see ensemble learning are capable of modeling more complex patterns in the data as compared to decision trees, leading to superior performance on the test dataset."]},{"cell_type":"markdown","metadata":{"id":"k0h4J88mNo0w","colab_type":"text"},"source":["### End of Lab 3\n","---\n"]}]}